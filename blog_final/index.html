<html>
<head>
<link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #0A0A2A;
		color: #E0E0E0;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		font-family: 'Roboto', 'Helvetica Neue', Helvetica, Arial, sans-serif;
		width: 100%; /* Ensure the container is full width */
		justify-content: left; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}
	.main-content-block {
		width: 70%;
		max-width: 1100px;
		background:  #1a1a3c; /* Dark blue gradient */
		border-left: 4px solid #5a8dbd;
		border-right: 4px solid #5a8dbd;
		margin: 0; /* Removes gaps between sections */
		padding: 8px;
		font-family: 'Roboto', 'Helvetica Neue', Helvetica, Arial, sans-serif;
		color: #E0E0E0;
		box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
		font-size: 20px;
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: 'Roboto', 'Helvetica Neue', Helvetica, Arial, sans-serif;
			padding: 5px;
	}
	.margin-right-block {
			font-family: 'Roboto', 'Helvetica Neue', Helvetica, Arial, sans-serif;
			font-size: 14px;
			width: 25%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 30px;
		font-family: 'Roboto', 'Helvetica Neue', Helvetica, Arial, sans-serif; /* Adds fallbacks */
		margin-top: 4px;
		margin-bottom: 10px;
	}

	h2 {
		font-size: 24px;
		font-family: 'Roboto', 'Helvetica Neue', Helvetica, Arial, sans-serif; /* Adds fallbacks */
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Roboto;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

	.table-container {
		display: flex;
		align-items: flex-start;
	}

	.table-caption {
		flex: 1;
		padding-left: 20px;
	}

</style>

	  <title>Geometric Algebra Transformers in the Wild!</title>
      <meta property="og:title" content="The Platonic Representation Hypothesis" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Roboto', 'Helvetica Neue', Helvetica, Arial, sans-serif; /* Adds fallbacks */">Investigating the Spatial Reasoning Capabilities of Geometric Algebra Transformers</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px"><a href="https://github.com/Ajay-M-create">Ajay Manicka</a></span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:18px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
			  <a href="#background">Background and Literature Review</a><br><br>
			  <a href="#Hypothesis">Hypothesis</a><br><br>
			  <a href="#experiments">Experiments</a><br><br>
			  <a href="#results">Results</a><br><br>
              <a href="#implications_and_limitations">Implications and limitations</a><br><br>
			  <a href="#acknowledgements">Acknowledgements</a><br><br>
			  <a href="#references">References</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
            <!--You can embed an image like this:-->
            <img src="./images/intro_graphic.jpg" width=800px/>
		    </div>
		    <!--div class="margin-right-block">
						Caption for the image.
		    </div:-->
		</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Introduction</h1>
						Morphometric diffusion models generate realistic 3D anatomical structures by modeling clinically relevant features
						like shape and size. An important application of these models is to give users control over desired features in
						generated 3D anatomic shapes. For example, it may be important to have control over features (volume, shape, etc.)
						of generated arteries when simulating coronary artery stent expansion.
						<br><br>
						Diffusion transformers can be used as the backbone of morphometric diffusion models since they model
						complex relationships in large datasets <a href="#ref_1">[1]</a> as opposed to the standard U-net architecture.
						They can be trained the learn the reverse diffusion process that the U-net would originally handle. While the authors demonstrate
						success in generating realistic 3D shapes superior to the U-net architecture, it is not clear how these models perform on spatial reasoning tasks of increasing complexity.
						Therefore, it is worth investigating if substituting the transformer backbone with a different architecture
						will yield improved performance on spatial reasoning tasks.
						<br><br>

            <img src="./images/Diffusion_model.png" width=800px/>

						<br><br>
						One alternative architecture is the Geometric Algebra Transformer (GATr), which may improve spatial reasoning by directly representing data as multivectors
						of the projective geometric algebra on complex shapes <a href="#ref_2">[2]</a>. Geometric algebra provides a structured framework that
						could enhance the diffusion model's understanding of 3D morphology, enabling a more accurate representation of
						complex structure. In the context of morphometric diffusion models, this could be useful for generating realistic 3D anatomic shapes which are intrinsically complex.
						<br><br>
						This project aims to investigate the spatial reasoning capabilities of GATr by testing the architecture on a variety of
						geometric reasoning tasks. It aims to characterize how the GATr's geometric reasoning capability scales with problem complexity.
						Here, we subject the GATr to a variety of challenging tasks that test its spatial reasoning capabilities compared to a standard transformer baseline. The official implementation extends the repository from <a href="#ref_2">[2]</a> and <a href="#ref_8">[8]</a> and can be found <a href="https://github.com/Ajay-M-create/GATr-experiments">here</a>.
		    </div>
		    <div class="margin-right-block">
						Figure 1. A diagram of the morphometric diffusion model architecture. The morphometric diffusion model generates realistic geometries with desired features. It can use a transformer backbone instead of a classical U-net architecture in its reverse diffusion process to learn the denoising process.
		    </div>
		</div>

		<div class="content-margin-container" id="background">
			<div class="margin-left-block">
			</div>

		<div class="main-content-block">
					<h1>Background and Literature Review</h1>

		<h2>Geometric Algebra G(3,0,1)</h2>
		Geometric Algebra provides a unified mathematical framework for representing and manipulating geometric objects and transformations.
		The specific algebra G(3,0,1) used in <a href="#ref_2">[2]</a> is a 4-dimensional algebra that extends 3D Euclidean space with a homogeneous coordinate, enabling projective geometry operations.
		In G(3,0,1), geometric objects such as points, lines, and planes are represented as multivectors.<br><br>

		The GATr begins by taking a set of geometric objects and embedding them as 16-dimensional multivectors in G(3,0,1). The data is then passed into the GATr architecture,
		which can be imagined as a mapping from one multivector space to another multivector space of the same dimension. The output multivector set allows for the extraction of the desired scalar output from it's scalar dimension (0th component).
		<br><br>

		The algebra provides natural operations for geometric transformations through the geometric product, which unifies inner and outer products.
		This allows rotations, translations, and other transformations to be represented as elements of the algebra and applied through <b>vector multiplication</b>, rather than through higher-dimensional and computationally expensive <b>matrix multiplication</b>.
		The geometric product between vectors a and b is defined as:<br><br>

          <center>
            <math xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
                <mi>a</mi>
                <mi>b</mi>
              </mrow>
              <mo>=</mo>
              <mrow>
                <mi>a</mi>
                <mo>&#x22C5;</mo>
                <mi>b</mi>
              </mrow>
              <mo>+</mo>
              <mrow>
                <mi>a</mi>
                <mo>&#x2227;</mo>
                <mi>b</mi>
              </mrow>
            </math>
          </center>


		<p>where the first term is the inner product and the second term is the outer product.</p>

		<p>Given a 4D orthogonal basis {e<sub>i</sub>}<sub>i</sub>, it holds that e<sub>0</sub>e<sub>0</sub> = 0, e<sub>i</sub>e<sub>i</sub> = 1 (i ≠ 0), and e<sub>i</sub>e<sub>j</sub> = -e<sub>j</sub>e<sub>i</sub> (i ≠ j).
		This means parallel basis vectors have magnitude 0, orthogonal basis vectors have magnitude 1, and two vectors anticommute (swap signs when multiplied in reverse order).
		All possible linearly independent geometric products span a 16-dimensional vector space of multivectors x ∈ G(3,0,1), structured as:</p>

		<p>x = (
			<span style="color: #4040FF">x<sub>s</sub></span>,
			<span style="color: #20B020">x<sub>0</sub>, x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub></span>,
			<span style="color: #B02020">x<sub>01</sub>, x<sub>02</sub>, x<sub>03</sub>, x<sub>12</sub>, x<sub>13</sub>, x<sub>23</sub></span>,
			<span style="color: #B0B020">x<sub>012</sub>, x<sub>013</sub>, x<sub>023</sub>, x<sub>123</sub></span>,
			x<sub>0123</sub>)</p>

		<p>where:</p>
		<ul>
			<li><span style="color: #4040FF">Scalar component</span></li>
			<li><span style="color: #20B020">Vector components</span></li>
			<li><span style="color: #B02020">Bivector components</span></li>
			<li><span style="color: #B0B020">Trivector components</span></li>
		</ul>


		<p>In this framework, both geometric objects and operations are represented as multivectors. Mathematically, we can represent any transformation as a series of reflections,
			and if the right basis is chosen, we can represent any reflection as a simple multiplication of geometric vectors. This is extremely powerful since it simplifies the procedure for performing complex
			geometric operations. Therefore, doing our analysis using Geometric Algebra should make more efficient use of computational resources for geometric operations. </p>
		</div>


		</div>

		<div class="content-margin-container">
			<div class="margin-left-block">
			</div>

			<div class="main-content-block">
				<p>Using this structure, different geometric objects can be represented as follows:</p>
				<table border="1" cellpadding="5" cellspacing="0" style="border-collapse: collapse; text-align: center; margin: 0 auto;">
					<thead>
						<tr>
							<th>Object / operator</th>
							<th>Scalar</th>
							<th>Vector<br>e<sub>0</sub>, e<sub>i</sub></th>
							<th>Bivector<br>e<sub>0i</sub>, e<sub>ij</sub></th>
							<th>Trivector<br>e<sub>0ij</sub>, e<sub>123</sub></th>
							<th>PS<br>e<sub>0123</sub></th>
						</tr>
					</thead>
					<tbody>
						<tr>
							<td>Scalar λ ∈ ℝ</td>
							<td>λ</td>
							<td>0</td>
							<td>0</td>
							<td>0</td>
							<td>0</td>
						</tr>
						<tr>
							<td>Plane w/ normal n ∈ ℝ³, origin shift d ∈ ℝ</td>
							<td>0</td>
							<td>d, n</td>
							<td>0</td>
							<td>0</td>
							<td>0</td>
						</tr>
						<tr>
							<td>Line w/ direction n ∈ ℝ³, orthogonal shift s ∈ ℝ³</td>
							<td>0</td>
							<td>0</td>
							<td>s, n</td>
							<td>0</td>
							<td>0</td>
						</tr>
						<tr>
							<td>Point p ∈ ℝ³</td>
							<td>0</td>
							<td>0</td>
							<td>0</td>
							<td>p, 1</td>
							<td>0</td>
						</tr>
						<tr>
							<td>Pseudoscalar μ ∈ ℝ</td>
							<td>0</td>
							<td>0</td>
							<td>0</td>
							<td>0</td>
							<td>μ</td>
						</tr>
						<tr>
							<td colspan="6"><strong>Transformations</strong></td>
						</tr>
						<tr>
							<td>Reflection through plane w/ normal n ∈ ℝ³, origin shift d ∈ ℝ</td>
							<td>0</td>
							<td>d, n</td>
							<td>0</td>
							<td>0</td>
							<td>0</td>
						</tr>
						<tr>
							<td>Translation t ∈ ℝ³</td>
							<td>1</td>
							<td>0</td>
							<td>½t, 0</td>
							<td>0</td>
							<td>0</td>
						</tr>
						<tr>
							<td>Rotation expressed as quaternion q ∈ ℝ⁴</td>
							<td>q<sub>0</sub></td>
							<td>0</td>
							<td>0,q<sub>i</sub></td>
							<td>0</td>
							<td>0</td>
						</tr>
						<tr>
							<td>Point reflection through p ∈ ℝ³</td>
							<td>0</td>
							<td>0</td>
							<td>0</td>
							<td>p, 1</td>
							<td>0</td>
						</tr>
					</tbody>
				</table>
			</div>

			<div class="margin-right-block">
				<strong>Table 1:</strong> Table provided in <a href="#ref_2">[2]</a>. This is a lookup table of different euclidean objects and operations represented as multivectors in G(3,0,1).
				It explains how the multivector embedding of geometric objects and operations can be entirely represented by multivectors.
			</div>
		</div>

		<div class="content-margin-container" id="Literature Review">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
		<h2>Previous Work</h2>
		    <p>
				Since the GATr is a new architecture, its characteristics have not been deeply studied.
				The original authors showed that the GATr could be used to efficiently represent geometric data and outperform other baselines in terms of error, data efficiency, and scalability for the nbody problem and artery shear and stress calculations <a href="#ref_2">[2]</a>. However, different avenues of research
				have been exploring the use of Geometric Algebra in different domains. One group has developed "Lab-GATr" which can learn transformations on large biomedical meshes <a href="#ref_3">[3]</a>, and another has developed "L-GATr" which embeds high energy physics data as multivectors <a href="#ref_5">[5]</a>.
				Another group extended GATr for large 3D meshes by employing cross-attention to project data onto a coarser vertex set <a href="#ref_4">[4]</a>.
				Geometric Algebra modeling itself has been explored by several groups in building machine learning networks <a href="#ref_6">[6]</a>, <a href="#ref_7">[7]</a>.

				<p>Currently there is a lot of interest in using GATr for a variety of tasks, especially in biomedical analysis and physics simulations.
				Despite there being results that GATr may perform well at scale, there is not yet a clear understanding of how the GATr's geometric reasoning capability scales with problem complexity.
				This project tries to bridge this gap in understanding by testing the GATr on a variety of geometric reasoning tasks of increasing complexity.</p>

				<p>Understanding how GATr performs depending on the complexity of the task is important to understand their strengths and weaknesses. In the context of morphometric diffusion models, understanding GATr's performance in reasoning through challenging tasks
				may give us insight into whether it can be useful in generating realistic complex 3D shapes.</p>
		    </p>
		</div>
	</div>

		<div class="content-margin-container" id="Hypothesis">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
		<h1>Hypothesis</h1>
			<p>
				<p>We would like to test the following hypothesis:</p>

				<p><b>Hypothesis:</b> While standard transformers may reason through simple geometric reasoning tasks well, the Geometric Algebra Transformer (GATr) architecture will perform better than standard transformers on geometric reasoning tasks of increasing complexity.</p>

				<p>This is because GATr is built on the framework of Geometric Algebra, which provides a more natural framework for representing and manipulating geometric objects and transformations.
				Standard transformers, on the other hand, are built on the framework of linear algebra and matrix multiplication, which may not be as natural for representing and manipulating geometric objects and transformations. </p>
			</p>
		</div>
	</div>

		<div class="content-margin-container" id="experiments">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Experiments</h1>
						<h2>Datasets</h2>
						<p>
						To test the hypothesis, a series of geometric reasoning tasks with increasing complexity were designed.
						First, three datasets were created: 10000 instances each of 5-point convex hulls, 10-point convex hulls, and 20-point convex hulls.
						The surface area, volume, and symmetry score of each shape was computed.
						The surface-area task was seen as the easiest since it can simply add up the areas of all planar faces.
						The volume task was set as the medium level difficulty task, since it would be harder to infer the volume from just the points,
						and the symmetry score was set as the hardest task since it requires reasoning about the arrangement of points in 3D space and more complex arithmetic.
						</p>

						<img src="./images/convex_hulls_comparison.png" width=800px/>

		    </div>
		    <div class="margin-right-block">
						Figure 2. A visualization of the 3D convex hulls used in the experiments.
						The top row shows the 5-point convex hulls, the middle row shows the 10-point convex hulls,
						and the bottom row shows the 20-point convex hulls. The volume, surface area, and symmetry score of each
						shape is shown above each corresponding convex hull.
		    </div>
		</div>

		<div class="content-margin-container" id="experiments_details">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
					<h2>Experimental Setup</h2>
					<p>
						Both transformer architectures were trained on 5,000 instances each of 5-point, 10-point, and 20-point convex hulls,
						with each model independently learning to predict volume, surface area, and symmetry scores.

						They each use 12 sequential transformer blocks to process geometric data, with more blocks enabling more complex transformations using a deeper network.
						The architectures differ in how they represent and process the input:

						<ul>
							<li><b>GATr:</b> Embeds the points in the convex hull as 16-dimensional multivectors through:
								<ul>
									<li>32 hidden and 16 output multivector channels for geometric operations</li>
									<li>256 hidden channels for scalar features</li>
									<li>Outputs a multivector with a scalar dimension representing the volume, surface area, or symmetry score through gradient backpropagation</li>
								</ul>
							</li>
							<li><b>Standard Transformer:</b> Processes 3D points directly through:
								<ul>
									<li>3 input channels (x,y,z coordinates)</li>
									<li>8 attention heads per block for learning point relationships</li>
									<li>128 hidden channels</li>
									<li>Outputs a single scalar value representing the volume, surface area, or symmetry score</li>
								</ul>
							</li>
						</ul>

						The same hyperparameters were used for all tasks, with each model trained for 10,000 steps. The MSE and MAE losses were calculated for the GATr and the standard transformer every 1000 steps.
						An accuracy plot was generated for the validation set every 1000 steps and also for the test set at the end of training. Results from the validation set were used for hyperparameter tuning initially,
						which is how the hyperparameters above were determined.

					</p>

		    </div>
		    <div class="margin-right-block">

		    </div>
		</div>



		<div class="content-margin-container" id="results">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Results</h1>

						Here, the major qualitative and quantitative results are summarized.
						Below are the accuracy plots for the validation set at steps 1000, 5000, and 10000,
						and the test set at the end of training across all tasks and hull sizes.

						The animations below show the evolution of the training process over three different steps for both the GATr and the standard transformer.
						The n_points variable represents the number of points in the convex hull used to train the model.

						Full results for every timestep can be found in the <a href="https://github.com/Ajay-M-create/GATr-experiments">Github</a> repository under:
						<pre style="margin-left: auto; margin-right: auto; width: fit-content;">tmp/
└── gatr-experiments/
    └── experiments/
        └── {task_name}/        # e.g. volume, surface_area, symmetry
            └── {model_run}/    # e.g. gatr_volume_10, transformer_volume_20
            	└── metrics/</pre>



						<center>
							<p><b>Hover over the videos below to see the animations.</b></p>
							<video class='transformer-video' loop autoplay muted style="width: 800px">
								<source src="./images/Transformer_acc.mp4" type="video/mp4">
							</video>

							<br><br>

							<video class='GATr-video' loop autoplay muted style="width: 800px">
								<source src="./images/GATr_acc.mp4" type="video/mp4">
							</video>
						</center>



		    </div>
		    <div class="margin-right-block">
				Figures 3 and 4. Evolution of accuracy for both the GATr and the Standard Transformer.
				The n_points variable represents the number of points in the convex hull used to train the model. The MSE and MAE losses are shown in the bottom left box of each plot.
			</div>
		</div>

		<div class="content-margin-container" id="results-table">
			<div class="margin-left-block">
			</div>
			<div class="main-content-block" style="display: flex; justify-content: center;">
			<table border="1" cellpadding="3" cellspacing="0" style="border-collapse: collapse; text-align: center; margin-right: 20px; font-size: 14px;">
				<thead>
					<tr>
						<th></th>
						<th colspan="2">Vol (5)</th>
						<th colspan="2">Vol (10)</th>
						<th colspan="2">Vol (20)</th>
						<th colspan="2">SA (5)</th>
						<th colspan="2">SA (10)</th>
						<th colspan="2">SA (20)</th>
						<th colspan="2">Sym (5)</th>
						<th colspan="2">Sym (10)</th>
						<th colspan="2">Sym (20)</th>
					</tr>
				</thead>
				<tbody>
					<tr>
						<td><b>Model</b></td>
						<td>GATr</td>
						<td>Tr</td>
						<td>GATr</td>
						<td>Tr</td>
						<td>GATr</td>
						<td>Tr</td>
						<td>GATr</td>
						<td>Tr</td>
						<td>GATr</td>
						<td>Tr</td>
						<td>GATr</td>
						<td>Tr</td>
						<td>GATr</td>
						<td>Tr</td>
						<td>GATr</td>
						<td>Tr</td>
						<td>GATr</td>
						<td>Tr</td>
					</tr>
					<tr>
						<td><b>MSE</b></td>
						<td>7.44e-5</td>
						<td style="background-color: #e6ffe6; color: black">2.00e-5</td>
						<td>4.43e-4</td>
						<td style="background-color: #e6ffe6; color: black">5.42e-5</td>
						<td>2.91e-3</td>
						<td style="background-color: #e6ffe6; color: black">8.28e-5</td>
						<td>1.99e-3</td>
						<td style="background-color: #e6ffe6; color: black">1.99e-4</td>
						<td>2.77e-2</td>
						<td style="background-color: #e6ffe6; color: black">3.57e-4</td>
						<td>1.91e-1</td>
						<td style="background-color: #e6ffe6; color: black">4.50e-4</td>
						<td style="background-color: #e6ffe6; color: black">4.08e-4</td>
						<td>5.42e-4</td>
						<td>1.05e-3</td>
						<td style="background-color: #e6ffe6; color: black">1.03e-3</td>
						<td>3.99e-3</td>
						<td style="background-color: #e6ffe6; color: black">1.47e-3</td>
					</tr>
					<tr>
						<td><b>MAE</b></td>
						<td>6.09e-3</td>
						<td style="background-color: #e6ffe6; color: black">3.35e-3</td>
						<td>1.83e-2</td>
						<td style="background-color: #e6ffe6; color: black">5.74e-3</td>
						<td>5.23e-2</td>
						<td style="background-color: #e6ffe6; color: black">7.21e-3</td>
						<td>4.08e-2</td>
						<td style="background-color: #e6ffe6; color: black">1.08e-2</td>
						<td>1.62e-1</td>
						<td style="background-color: #e6ffe6; color: black">1.49e-2</td>
						<td>4.33e-1</td>
						<td style="background-color: #e6ffe6; color: black">1.69e-2</td>
						<td style="background-color: #e6ffe6; color: black">1.34e-2</td>
						<td>1.58e-2</td>
						<td>2.48e-2</td>
						<td style="background-color: #e6ffe6; color: black">2.47e-2</td>
						<td>5.01e-2</td>
						<td style="background-color: #e6ffe6; color: black">3.04e-2</td>
					</tr>
				</tbody>
			</table>
			</div>

			<div class="table-caption" style="flex: 1; padding-left: 20px; font-size: 14px;">
				<strong>Table 2:</strong> Comparison of Mean Squared Error (MSE) and Mean Absolute Error (MAE) between GATr and standard Transformer models across different tasks and point cloud sizes. Cells highlighted in light green indicate the better performing model for each metric and configuration.
				The results show that the standard Transformer had lower MSE and MAE for all tasks, and this discrepancy increased with number of points.
			</div>
		</div>

		<div class="content-margin-container" id="implications_and_limitations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Implications and limitations</h1>
						Contrary to our hypothesis, the standard transformer significantly outperformed the GATr on the simpler geometric tasks of surface area and volume prediction. However, the results for the symmetry score task showed mixed results that may provide insight into how these architectures handle increasing complexity.

						<h2>Analysis of Numerical Results</h2>
						Looking at the numerical results in Table 2:

						<ul>
							<li><b>Volume and Surface Area (Simple Tasks):</b> The standard transformer consistently outperformed the GATr by a significant margin. For 20-point hulls, the GATr's surface area MSE (1.91e-1) was several orders of magnitude higher than the Transformer's (4.50e-4). Similarly for volume, the GATr's MSE (2.91e-3) was much higher than the Transformer's (8.28e-5).</li>

							<li><b>Symmetry Score (Complex Task):</b> Interestingly, we observed mixed performance on the symmetry score prediction:
								<ul>
									<li>For 5-point hulls, the GATr performed slightly better than the standard transformer</li>
									<li>At 10 points, both models showed comparable performance</li>
									<li>At 20 points, both models struggled significantly, though the transformer maintained a slight edge (MSE: 1.47e-3 vs 3.99e-3)</li>
								</ul>
							</li>
						</ul>

						<p>These results suggest an interesting pattern: while the standard transformer appears superior for simpler geometric calculations, the relative performance on the more complex symmetry task shows potential promise for the GATr architecture. The fact that the GATr performed better on the symmetry task with smaller point clouds, before degrading with larger ones, hints that its geometric algebra representation might offer advantages for certain types of complex geometric reasoning.
						</p>

						<p>One possible explanation is that the multivector representation, while theoretically powerful, may introduce unnecessary complexity for straightforward geometric calculations like volume and surface area, which is underutilizing the 16-dimensional representation for a 3-D problem. However, for more sophisticated geometric reasoning tasks, this same representation might become advantageous if properly leveraged. The mixed performance on the symmetry task, particularly the GATr's superior performance with smaller point sets, suggests that there might be specific problem domains where the GATr's geometric algebra approach could prove more effective than standard transformers.
						</p>

						<h2>Limitations</h2>

						<p>In this experiment, we did not account for proper control of resources rigorously. An effort was made to choose model hyperarameters that would make the two models more comparable, but there are clearly hyperparameter choices that would make one model perform better than the other.
						During training, the GATr took 8-10x as long as the standard transformer to train and an order of magnitude more parameters available. So, the comparison is likely to not be fair. However, this was a similar setup used in the original GATr paper for nbody simulations, so it is a good starting point for this experiment.
						For future work, controlling for resources rigorously would be important to make a fair comparison between the two models and better assess the GATr's performance.
						</p>

						<h2>Future Work</h2>
						<p>This preliminary study suggests that GATr may not be the best choice for a transformer architecture within a morphometric diffusion model when reasoning through simple geometric tasks. However, they still may be useful in more challenging tasks like the symmetry score prediction, nbody simulation, and arterial mesh tasks.
						Therefore, the next step would be to explore how the multivectors evolve over the course of training to see if the GATr is making use of the extra representational power of the multivectors.
						Next, it would be interersting to implement the GATr in a morphometric diffusion model and visualize how each architecture causes differences in generated shapes.</p>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>



		<div class="content-margin-container" id="acknowledgements">
			<div class="margin-left-block">
			</div>
		<div class="main-content-block">
					<h1>Acknowledgements</h1>
					I would like to thank the 6.7960 course staff for their guidance and support throughout this project. I would also like to thank my labmates for their helpful discussions and feedback.
		</div>
		<div class="margin-right-block">
		</div>
	</div>

		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto; background: transparent"><br>
							<h1>References:</h1><br><br>
							<a id="ref_1"></a>[1] <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.pdf">Scalable diffusion models with Transformers</a>, Peebles, William, and Saining Xie. "Scalable diffusion models with transformers." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.<br><br>
							<a id="ref_2"></a>[2] <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/6f6dd92b03ff9be7468a6104611c9187-Paper-Conference.pdf">Geometric Algebra Transformer</a>, Johann Brehmer, Pim De Haan, Sönke Behrends, and Taco S Cohen. "Geometric algebra transformer." Advances in Neural Information Processing Systems, 36, 2024.<br><br>
							<a id="ref_3"></a>[3] <a href="https://github.com/sukjulian/lab-gatr">LaB-GATr</a>, Suk, Julian, Baris Imre, and Jelmer M. Wolterink. "LaB-GATr: geometric algebra transformers for large biomedical surface and volume meshes." International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2024.<br><br>
							<a id="ref_4"></a>[4] <a href="https://openreview.net/pdf?id=T2bBUlaJTA">Geometric algebra transformers for large 3D meshes via cross-attention</a>, Suk J, De Haan P, Imre B, Wolterink JM. Geometric algebra transformers for large 3d meshes via cross-attention. InICML 2024 Workshop on Geometry-grounded Representation Learning and Generative Modeling 2024.<br><br>
							<a id="ref_5"></a>[5] <a href="https://arxiv.org/pdf/2405.14806">Lorentz-Equivariant Geometric Algebra Transformers for High-Energy Physics</a>, Spinner J, Bresó V, de Haan P, Plehn T, Thaler J, Brehmer J. Lorentz-Equivariant Geometric Algebra Transformers for High-Energy Physics. arXiv preprint arXiv:2405.14806. 2024 May 23.<br><br>
							<a id="ref_6"></a>[6] <a href="https://arxiv.org/pdf/2110.02393.pdf">Geometric algebra attention networks for small point clouds</a>, Spellings, Matthew. "Geometric algebra attention networks for small point clouds." arXiv preprint arXiv:2110.02393 (2021).<br><br>
							<a id="ref_7"></a>[7] <a href="https://arxiv.org/pdf/2206.03589.pdf">Using a graph transformer network to predict 3d coordinates of proteins via Geometric Algebra modelling</a>, Pepe, Alberto, Joan Lasenby, and Pablo Chacon. "Using a graph transformer network to predict 3d coordinates of proteins via Geometric Algebra modelling." International Workshop on Empowering Novel Geometric Algebra for Graphics and Engineering. Cham: Springer Nature Switzerland, 2022.<br><br>
							<a id="ref_8"></a>[8] <a href="https://arxiv.org/abs/2311.04744">Euclidean, Projective, Conformal: Choosing a Geometric Algebra for Equivariant Transformers</a>, de Haan P, Cohen T, Brehmer J. Euclidean, Projective, Conformal: Choosing a Geometric Algebra for Equivariant Transformers. Proceedings of the 27th International Conference on Artificial Intelligence and Statistics. 2024;27.<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
            <!-- margin notes for reference block here -->
		    </div>
		</div>



	</body>

</html>
